{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Install\n",
    "# pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Torch vision\n",
    "# pip install torchvision\n",
    "\n",
    "#  !pip install into Jupyter\n",
    "\n",
    "# pip install opencv-python \n",
    "# pip install tqdm\n",
    "\n",
    "# Jupyter notebook themes\n",
    "# pip install jupyterthemes\n",
    "\n",
    "##original theme\n",
    "# jt -r\n",
    "\n",
    "#reset theme with tool bar\n",
    "# Toolbar Visible -T\n",
    "# Name & Logo Visible -N\n",
    "# Kernel Logo Visible -kl\n",
    "# their defaults are set to none\n",
    "\n",
    "#change the theme need to reset JN at first but then you just have to refresh \n",
    "# !jt -t gruvboxd -T -N -kl\n",
    "\n",
    "## black theme\n",
    "#jt -t chesterish\n",
    "##List of Themes\n",
    "# !jt -l\n",
    "\n",
    "# onedork\n",
    "# grade3\n",
    "# oceans16\n",
    "# chesterish\n",
    "# monokai\n",
    "# solarizedl\n",
    "# solarizedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentdex building our neural network on youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model \n",
    "# obj oriented programming\n",
    "import torch.nn as nn\n",
    "#functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=torch.Tensor([5,3])        #tensor is array\n",
    "y=torch.Tensor([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "print(X*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "X=torch.zeros([2,5])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random variables in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7216, 0.3050, 0.2310, 0.2371, 0.7292],\n",
      "        [0.5941, 0.8966, 0.2333, 0.4403, 0.2146]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.rand([2,5])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7216, 0.3050, 0.2310, 0.2371, 0.7292, 0.5941, 0.8966, 0.2333, 0.4403,\n",
      "         0.2146]])\n"
     ]
    }
   ],
   "source": [
    "# reshape something--view method\n",
    "#flatten operations 2,5 cannot go into nn inot 1 by 10\n",
    "# need to reassign\n",
    "y=y.view([1,10])\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision is cheating with built in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally use \"\", transforms.Compose([all transforms apply to data])\n",
    "#have to convert to tensor--transforms.ToTensor()\n",
    "\n",
    "# This gets data downloaded into variables \n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test =datasets.MNIST(\"\", train=False, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load obj\n",
    "# batch size-how many at a time pass to model\n",
    "# shuffle-\n",
    "#mnist-hand drawn digits from 0 to 9, 28 by 28 image\n",
    "\n",
    "trainset=torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset=torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([2, 0, 5, 2, 5, 6, 7, 8, 3, 7])]\n"
     ]
    }
   ],
   "source": [
    "# iterate\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "#last temp variable in a for loop can still be accessed\n",
    "# data is a tensor object with x and y , second x,y tensors with labels\n",
    "X,y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)\n",
    "# not a typical image on grayscale converted into tensor \n",
    "#(that would be 28 by 28 but pytorch wants that 1 there)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view is like reshape but not inplace\n",
    "# plt.imshow(data[0][0].view(28,28))\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# balancing-model finds shorter path, optimizer decreases loss as easy as possible\n",
    "# data sets include data from 0 to 9 but if 60% data set is 3s this will be the easiest way to preditc\n",
    "total =0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    # xs and ys stored in variables\n",
    "    Xs,ys =data\n",
    "    for y in ys:\n",
    "        # convert into dictionary key\n",
    "        # how many samples for each number\n",
    "        counter_dict[int(y)] +=1\n",
    "        total +=1\n",
    "print(counter_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "# gives the percentage distribution\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):      # initialization method is not run\n",
    "        super().__init__()   # super correspons to nn.Module to initialize\n",
    "        # want initalization to run with parent class inheriting from\n",
    "        # define fully connected layers to nn\n",
    "        self.fc1 = nn.Linear(28*28, 64)   #fc1 is fully connected 1st layer, input and output to each layer, input is rows of image pizels flatten\n",
    "        self.fc2 = nn.Linear(64, 64)   #nn.linear is fully connected flattened\n",
    "        self.fc3 = nn.Linear(64, 64)   # 784 is 28*28, output is 3 layers of 64 layers so out put is 64, each layer has input coming into layer (flattened image) output is 64\n",
    "        self.fc4 = nn.Linear(64, 10)   # go from image to code, 64 is extent to hidden layer\n",
    "        # fc2 takes in 64, fc3 takes in 64 but can output anythin\n",
    "        # 10 classes ( 10 neurons)\n",
    "        # without super error-cannot assign module before Module.__iinit__() call\n",
    "    \n",
    "    # dont have path for data to take for layer feed forward layer from one side ot the other side\n",
    "    #feed forward\n",
    "    def forward(self, X):\n",
    "        X= F.relu(self.fc1(X))      # X is going to pass through the layers\n",
    "        X= F.relu(self.fc2(X))      # relu activation layer using F.relu-whether the neuron is firing-keeps outputs from giving crazy numbers\n",
    "        #activation funciton is to make it a nonlinear function\n",
    "        X= F.relu(self.fc3(X))\n",
    "        X= self.fc4(X)   #just pass the data, activaiton on 64 only , dont want relu , want one neuron fully fired [0,0,1] (dog, cat, human)\n",
    "        \n",
    "        return F.log_softmax(X, dim=1) #dimenison apply softmax for multiclass = dim=1 similar to axis probablity distribution goes to 1--- what do we want to sum to 1? care about classes themselves-distributing across the batches to 1---batch of tensors to sum to 1\n",
    "\n",
    "net =Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has to be flattened first\n",
    "# -1 specifies unkonwn shape\n",
    "X = torch.rand((28,28))\n",
    "X=X.view(-1,28*28)\n",
    "# output=net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4401, -2.2816, -2.2858, -2.3508, -2.2744, -2.2471, -2.3368, -2.3303,\n",
       "         -2.2261, -2.2697]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual predictions\n",
    "# grad_fn---passes data through\n",
    "output=net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# net.parm is everything is adjustable in model, later latyers are specific to tasks\n",
    "#transfer learning\n",
    "    # first few layers small general\n",
    "    # later layers specific\n",
    "# transfer learning-freeze first few layers only let model adjust weights on last layers\n",
    "\n",
    "#learning rate-optimization curve-converge to the bottom of the curve\n",
    "    # size of step to lower the loss with certain szie steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0352, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer=optim.Adam(net.parameters(), lr=0.001 ) # 1e-1\n",
    "#decaying learning rate-larege initial steps but smaller and smaller to converge at bottom\n",
    "#iterate over data and pass through model\n",
    "#epoch is full pass through data\n",
    "EPOCHS=3\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a batch of feture sets and labels\n",
    "        X, y = data #unpack\n",
    "        #There are rows of pixels that we need to flatten\n",
    "#         print(X[0])\n",
    "#         print(y[0])\n",
    "#         break\n",
    "        #everytime calculate loss, optimize model, make gradients\n",
    "        #everytime pass data through nn\n",
    "        # zero the gradient to prevent batches added together\n",
    "        #only pass through one set of labels but still get benefit of batch training\n",
    "        # gradients contain loss-optimizer uses gradients for weights\n",
    "        net.zero_grad()\n",
    "        #batch data decreases training time,dont want to pass entire dataset\n",
    "        #bt batch size of 32 and 128 helps to generalize-weak GPU only pass through one set of labels\n",
    "        output=net(X.view(-1, 28*28))\n",
    "        #caluclate how wrong\n",
    "        #loss calculation-numerical-one hot vectors= output-array is one is on, zero is off[0,1,1]-mean squared error, regission something else\n",
    "        #loss calculation-scaler-singular value-different loss metric-not a vector-nll_loss\n",
    "        loss=F.nll_loss(output, y)\n",
    "        #which loss is applicable to each environment and what is loss doing\n",
    "        #back propogate the loss\n",
    "        # \n",
    "        loss.backward()\n",
    "        #adjust weights for us\n",
    "        optimizer.step()\n",
    "    # expect loss to be close to zero\n",
    "    print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "#validate data, dont need gradients\n",
    "#how right or wrong the model is\n",
    "\n",
    "with torch.no_grad():\n",
    "    #net.train and net.eval()-dictabte training or eval model before--old\n",
    "    for data in trainset:\n",
    "        X,y=data\n",
    "        output=net(X.view(-1,784))\n",
    "        #want to compare argmax\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) ==y[idx]:\n",
    "                correct +=1\n",
    "            total+=1\n",
    "# for every prediciton does it match target value\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plot\n",
    "# plot.imshow(X[0].view(28,28))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Premade data set of raw inmages to build new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional nn-new type of layer\n",
    "#use them for image tasks-but convolutional nn outpreform recural nn\n",
    "#this is 1D convolutional nn\n",
    "\n",
    "#convolutional nn\n",
    "#pass the image in its form so dont have ot flatten image\n",
    "#accepts 2D input and even 3D input-so could put in 3D printing models\n",
    "#its pixels-2D-apply convolution to locate features of image\n",
    "#breaks into window(kernal)-looks for feature in 3x3 pixels\n",
    "## nn works on numbers and not strings-but convolutional nn-first layer finds things like edges, curves, corners\n",
    "#then passes through another layer to find more ocmplex features like circles, etc..\n",
    "#finds features then slides over the entire image and condenses the image\n",
    "# find 30 features of image\n",
    "\n",
    "#after convolution get a new condensed version with numbers of the features\n",
    "#pool using a window(max pooling is the most common)-takes the max value, then slides and does it again\n",
    "\n",
    "#really simplifying image looking for features-have 2, 3, 4 convolutional layers\n",
    "#first layer-just basic features-edges, cormers,curves-combination of pixels\n",
    "#next layer is combination-sees combination of curves, edge, and curves to find circles, sqares, etc\n",
    "# next layer is finding even more combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12501/12501 [03:25<00:00, 60.94it/s]\n",
      "  0%|                                                                                | 4/12501 [00:00<06:51, 30.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 12501/12501 [01:26<00:00, 145.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats:  12476\n",
      "Dogs:  12470\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA=True # set to true once then back to false unless you want \n",
    "class DogsVSCats():\n",
    "    IMG_SIZE=50 # make images 50 by 50, images are different sizes and shapes\n",
    "    #want them all to be normalized set the smallest dimentions to 50\n",
    "    # go and resize, pad, rotate, flip, \n",
    "    CATS=\"PetImages/Cat\"\n",
    "    DOGS=\"PetImages/Dog\"\n",
    "    LABELS={CATS:0, DOGS:1}\n",
    "    training_data=[]\n",
    "    catcount=0\n",
    "    dogcount=0\n",
    "    \n",
    "    def make_training_data(self):\n",
    "        # iterate over cats and dogs-keys of dictionary\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            #iterate over all the images in the directories\n",
    "            for f in tqdm(os.listdir(label)): #tqdm-progress bar where we are(directory)\n",
    "            #f is the file name but want the path\n",
    "                try:\n",
    "                    path=os.path.join(label,f)\n",
    "                    img=cv2.imread(path, cv2.IMREAD_GRAYSCALE) #converitng to grayscale, not necessary butdense layers,nn will need to be flat \n",
    "                    # convolutionlal layer will not need to be flat\n",
    "                    # color adds channels but not dimensions but its added data that we dont need- is this relevant\n",
    "                    img=cv2.resize(img,(self.IMG_SIZE, self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]]) # convert scaler values to one hot vector (0 or 1) but in an index- use np.eye(5) to make 5x5 with 1s\n",
    "                    if label ==self.CATS:\n",
    "                        self.catcount +=1\n",
    "                    elif label ==self.DOGS:\n",
    "                        self.dogcount+=1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    #print(str(e)) # to identify errors\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print(\"Cats: \", self.catcount)\n",
    "        print(\"Dogs: \", self.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats=DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.eye(10)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.load(\"training_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 75,  82,  78, ..., 226, 213, 225],\n",
      "       [ 80,  78,  92, ..., 185, 190, 237],\n",
      "       [ 90,  95,  93, ..., 203, 199, 203],\n",
      "       ...,\n",
      "       [103, 115, 118, ..., 103,  94, 101],\n",
      "       [111, 112, 124, ..., 104,  99,  96],\n",
      "       [100, 119, 123, ..., 100,  99,  99]], dtype=uint8)\n",
      " array([0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(training_data[1][0], cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) #input, 32 convolutional features, kernal size (5x5 window to roll over the data to find features)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) #2d layers (x and y axis)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5) \n",
    "        #has to output into a linear layer as a distrbution of predictions\n",
    "        #tensorFlow-keras has a flatten function\n",
    "        x = torch.randn(50,50).view(-1,1,50,50) # how ever images will be in tensor form input image: 1x50x50\n",
    "        self._to_linear = None\n",
    "        self.convs(x) \n",
    "        #need to run data into con1, con2, con3 then check the shape and multiple dimensions.\n",
    "        #comment out self.fc1 and 2 then pass random data andn pass through self.convs(x)\n",
    "        #have to know self._to_linear with quick forward pass\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "        \n",
    "        \n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 2,2 is the shape of the pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "#         print(x[0].shape)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "         #X is coming in as a batch of data\n",
    "        # return x to create forward \n",
    "        return x\n",
    "#         self.fc1=nn.Linear(self._to_linear, 512) #have to find the input\n",
    "#         self.fc2=nn.Linear(512, 2) #2 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear) #reshape to be flattened, we need to know the number before we call it in the initailiation\n",
    "        # have to know this flattened number\n",
    "        x = F.relu(self.fc1(x)) #first fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)# dont need activation layer bc just a bunch of vectors being multiplied together\n",
    "        # dim=1 because of the batches, x is a batch of Xs, 0th dimension wouldd be all dimesntions, want distribuition (0 to 1 percentatge) distributed across cat and dog classifications\n",
    "        # this one can fit, train on x and it would work\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.flatten(input, start_dim=0, end_dim=-1) # Pytorch has its own flatten method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#mean squared error for 0 and 1\n",
    "loss_function = nn.MSELoss()\n",
    "#seperate out Xs and ys-make them a tensor- i[0] just the Xs\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "# scaling imadry, pixel bt 0 and 255 and want them to be between 0 and 1\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "# split train and test data by simply slicing out val size\n",
    "VAL_PCT = 0.1 #tests against 10% of data set\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size) # validation tests size int slice\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 225/225 [01:56<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.22874847054481506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# determine batch size, memory error-lower batch size\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "# Slices through training data, first batch is 0 to 100 index through training dataset \n",
    "for epoch in range(EPOCHS):\n",
    "    #take steps that are the size of BATCH_SIZE\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "        #tqdm is progress bar\n",
    "#         print(i, i+BATCH_SIZE) \n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50) #reshape with view\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        #zero gradients\n",
    "        net.zero_grad()\n",
    "        #model.zero_grad\n",
    "#         optimizer.zero_grad #above-optim.Adam-net parameters-no diff bt net.zer or optimizer.zer---might have model that might have diff optimizers on diff sizdez, muliltiple potimzers\n",
    "        #want to use the specific optimizer (adam, etc) whic hway do you need to be explitc, just this optimzizer, just need to know which is which\n",
    "        outputs = net(batch_X) #gives outputs to calculate loss-use loss function above\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2494/2494 [00:12<00:00, 193.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3)) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
